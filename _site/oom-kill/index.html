<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  <title>How to Troubleshoot OOM Issues in Go Applications Running on Kubernetes | César Sepúlveda</title>

  
  <link rel="canonical" href="https://medium.com/@csepulvedab/how-to-troubleshoot-oom-issues-in-go-applications-running-on-kubernetes-149e8bb104ac" />
  

  <!-- For Atom Feed-->
  

  <meta name="msvalidate.01" content="D8B989D9516D49E8D4EFD5141B2950A9" />
  <meta name="description" content="Kubernetes provides valuable mechanisms to prevent nodes from being affected when certain pods consume excessive CPU or memory resources, thanks to cgroups. ..."/>
  <meta name="keywords" content="software,engineer,devops,csepulveda,kubernetes go"/>
  <meta property="og:type" content="article"/>
  <meta property="og:title" content="How to Troubleshoot OOM Issues in Go Applications Running on Kubernetes | César Sepúlveda"/>
  <meta property="og:description" content="Kubernetes provides valuable mechanisms to prevent nodes from being affected when certain pods consume excessive CPU or memory resources, thanks to cgroups. ..."/>
  
  <meta property="og:image" content="http://localhost:4000/assets/img/2024-11-11-oom-kill/oom.png">
  
  <meta property="og:url" content="http://localhost:4000/oom-kill/"/>
  <meta name="twitter:card" content="summary_large_image"/>

  <link rel="icon" href="/assets/img/favicon.png">

  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/balloon-css/0.5.0/balloon.min.css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.1/css/bootstrap.min.css" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.1.0/css/all.css" integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous">
  <link href="//fonts.googleapis.com/css?family=PT+Sans|Playfair+Display:700|Montserrat:300,400" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/main.css">
</head>

  <body class="d-none">
      <div class="flex-container">
    <header class="main-header">
    <div class="wrapper">
        <div class="header-flex">
            <div class="left-header-container">
                <a href="/"> Home </a>
            </div>
            <div class="center-header-container">
                <div class="logo">
                    <div class="author-name">César Sepúlveda</div>
                    <div class="author-title">Infrastructure Engineer</div>
                </div>
                <ul class="contact-links">
    
    <li class="email" data-balloon="Email" data-balloon-pos="left"><a href="mailto:cesar.sepulveda@gmail.com"><i class="far fa-envelope"></i></a></li>
    

    
    <li class="linkedin" data-balloon="LinkedIn" data-balloon-pos="left"><a href="https://in.linkedin.com/in/csepulvedab" target="_blank"><i
            class="fab fa-linkedin"></i></a></li>
    

    
    <li class="github" data-balloon="GitHub" data-balloon-pos="right"><a href="http://github.com/csepulveda" target="_blank"><i
            class="fab fa-github"></i></a></li>
    

    
    

    
    
</ul>
            </div>
            <div class="right-header-container">
                <a href="/about"> About </a>
            </div>
        </div>
    </div>
</header> <!-- End Header -->

    <article class="article-page">
        <div class="page-image">
            
            <div class="cover-image" style="background: url(/assets/img/2024-11-11-oom-kill/oom.png) center no-repeat; background-size: cover;"></div>
            
        </div>
        <div class="wrapper">
            <div class="page-content">
                <div class="header-page">
                    <h1 class="page-title">How to Troubleshoot OOM Issues in Go Applications Running on Kubernetes</h1>
                    <div class="page-date"><time datetime="2024-11-11 01:00:00 -0300">Nov 11, 2024</time></div>
                    <div class="page-links">
                    
                    
                    
                    
                    </div>
                    <div class="page-share">
    <a href="https://twitter.com/intent/tweet?text=How to Troubleshoot OOM Issues in Go Applications Running on Kubernetes&url=http://localhost:4000/oom-kill/" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a>
    <!--<a href="https://facebook.com/sharer.php?u=http://localhost:4000/oom-kill/" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fab fa-facebook" aria-hidden="true"></i></a>-->
    <a href="https://www.linkedin.com/shareArticle?url=http://localhost:4000/oom-kill/" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a>
    <!--<a href="https://plus.google.com/share?url=http://localhost:4000/oom-kill/" title="Share on Google+" rel="nofollow" target="_blank"><i class="fab fa-google-plus-g" aria-hidden="true"></i></a>-->
</div>
                </div>
                <p>Kubernetes provides valuable mechanisms to prevent nodes from being affected when certain pods consume excessive CPU or memory resources, thanks to cgroups. However, memory limits can pose unique challenges. Unlike CPU limits, which can throttle usage over time, memory cannot be gradually restricted. When a container reaches its memory limit, it receives a SIGKILL signal, causing the container to restart.</p>

<p>In my current role, we addressed this by implementing a solution for all our Go applications using the pprof library. By enabling a listening port with pprof, we can retrieve memory heap data from any running process via the /debug/pprof/heap endpoint, which returns a memory heap dump.</p>

<p>We also developed a separate application that continuously monitors memory usage. This application queries the Kubernetes metrics server to check if any container is using more than 80% of its configured memory limit. If this threshold is exceeded, it connects to the pod’s IP on a pre-configured port, retrieves the heap memory data, and stores it in S3 for further analysis using the go tool pprof.</p>

<p>This approach works well for applications with gradual memory leaks, where memory accumulation occurs slowly over time. In such cases, the monitoring service can detect high memory usage and generate a heap dump before the application receives a SIGKILL due to an OOM event.</p>

<p>However, we often encounter cases where memory leaks are more aggressive, with containers jumping from 50 MB to over 1 GB in just a few seconds.</p>

<p>To handle these cases, we implemented a different approach using a library developed by <a href="https://github.com/ricardomaraschini/oomhero">Ricardo Maraschini</a>. This library allows a sidecar container to monitor process metrics in real-time, querying statistics every second instead of the 30-second intervals we had with the metrics server.</p>

<p>We built on this with our custom solution at <a href="https://github.com/csepulveda/oom-heap-dumper">https://github.com/csepulveda/oom-heap-dumper</a> which enhances Maraschini’s oomhero library by adding functionality to check the app’s listening ports, download the heap from each port, and store it in S3.</p>

<p>Compared to the previous approach, this method introduces some security considerations and may not be suitable for every pod in the system. For this solution to work, we need to enable shareProcessNamespace and set the oom-heap-dumper container privileges with securityContext.privileged: true.</p>

<p>This updated approach allows us to capture a heap memory dump and send it to S3 for later analysis, even when memory spikes occur rapidly.</p>

<h2 id="practical-example">Practical Example</h2>
<p>Consider the following code, which forces a memory leak by doubling memory usage with each iteration:
<a href="https://github.com/csepulveda/go-memory-leak/tree/v0.0.3">code</a></p>

<p>When running this application in a container on our Kubernetes cluster, we observe the following results:</p>

<h3 id="deployment-configuration">Deployment Configuration:</h3>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/csepulveda/go-memory-leak:v0.0.3</span>
          <span class="na">resources</span><span class="pi">:</span>
            <span class="na">requests</span><span class="pi">:</span>
              <span class="na">memory</span><span class="pi">:</span> <span class="s">64Mi</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">250m</span>
            <span class="na">limits</span><span class="pi">:</span>
              <span class="na">memory</span><span class="pi">:</span> <span class="s">240Mi</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">250m</span>
</code></pre></div></div>

<h3 id="memory-stats">Memory Stats:</h3>
<p><img src="/assets/img/2024-11-11-oom-kill/memory-stats.png" alt="memory-stats" title="memory-stats" /></p>

<h3 id="app-logs-and-oom-events">App Logs and OOM events:</h3>
<p><img src="/assets/img/2024-11-11-oom-kill/memory-events.png" alt="memory-events" title="memory-events" /></p>

<p><img src="/assets/img/2024-11-11-oom-kill/memory-events-1.png" alt="memory-events-1" title="memory-events-1" /></p>

<p>As shown, monitoring the metric server directly and attempting to take a memory dump when usage exceeds 80% is challenging. With each cycle doubling memory usage, we go from 50% to nearly 100% in one jump. These rapid changes make it difficult for the metric server to capture and respond to fluctuations in real-time.</p>

<p>To address this, our first step is to add pprof middleware to our application.</p>

<h3 id="using-the-sidecar-to-auto-generate-the-heap-file">Using the Sidecar to auto generate the Heap File</h3>

<p>Code Comparison:</p>

<p><a href="https://github.com/csepulveda/go-memory-leak/compare/v0.0.3…v0.0.4">https://github.com/csepulveda/go-memory-leak/compare/v0.0.3…v0.0.4</a>
<img src="/assets/img/2024-11-11-oom-kill/code.png" alt="code" title="code" /></p>

<p>This enables us to retrieve the application’s memory heap, allowing us to analyze it locally. For example, if the app is running on port 8080, we can launch the pprof web interface on port 8081 to analyze the heap file:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>go tool pprof <span class="nt">-http</span><span class="o">=</span>:8081 localhost:8080/debug/pprof/heap
</code></pre></div></div>

<p>Through this analysis, we can see the memory leak originates within the main function, specifically when calling serializeJSON.
<img src="/assets/img/2024-11-11-oom-kill/oom.png" alt="oom" title="oom" /></p>

<p><img src="/assets/img/2024-11-11-oom-kill/oom-1.png" alt="oom-1" title="oom-1" /></p>

<p>For this example, replicating the memory leak behavior is straightforward. However, in cases where memory leaks occur sporadically, capturing a memory dump at the exact moment of high memory usage is essential. This is where our sidecar solution becomes invaluable.</p>

<h2 id="deployment-configuration-to-add-the-sidecar">Deployment Configuration to Add the Sidecar:</h2>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">go-memory-leak</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">shareProcessNamespace</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/csepulveda/go-memory-leak:v0.0.4</span>
          <span class="na">resources</span><span class="pi">:</span>
            <span class="na">requests</span><span class="pi">:</span>
              <span class="na">memory</span><span class="pi">:</span> <span class="s">64Mi</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">250m</span>
            <span class="na">limits</span><span class="pi">:</span>
              <span class="na">memory</span><span class="pi">:</span> <span class="s">130Mi</span>
              <span class="na">cpu</span><span class="pi">:</span> <span class="s">250m</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">oom-heap-dump</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/csepulveda/oom-heap-dumper:v0.0.5</span>
          <span class="na">securityContext</span><span class="pi">:</span>
            <span class="na">privileged</span><span class="pi">:</span> <span class="no">true</span>
          <span class="na">env</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">CRITICAL</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">70"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">COOLDOWN</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">10s"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">WATCH_TIME</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">100ms"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">BUCKET</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">bucket-name"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_REGION</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">us-west-1"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_ACCESS_KEY_ID</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">xxx"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_ACCESS_KEY</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">yyy"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SESSION_TOKEN</span>
            <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">zzz"</span>
</code></pre></div></div>

<p>In this YAML, the keys and bucket name are masked for security, but it’s possible to pass credentials directly as environment variables or use a service account that grants write permissions to the bucket where memory heaps will be stored. For more on setting permissions, check out my post on configuring IAM roles with service accounts: <a href="https://medium.com/@csepulvedab/three-approaches-to-defining-iam-roles-in-kubernetes-on-eks-a3094201333f">how to define iam roles on eks</a></p>

<p>When the app is running, you’ll see logs from our oom-dumper once it reaches the 70% memory threshold set as the maximum memory usage.</p>

<p><img src="/assets/img/2024-11-11-oom-kill/mem-1.png" alt="mem-1" title="mem-1" />
<img src="/assets/img/2024-11-11-oom-kill/mem-2.png" alt="mem-2" title="mem-2" /></p>

<h2 id="retrieving-and-visualizing-the-heap">Retrieving and Visualizing the Heap</h2>

<p>Download the heap from S3 and view it locally:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">aws s3 cp s3://xxxx/go-memory-leak-54d6d47d7-6hl64-7-8080-2024-11-11-03-49.heap .</span>
<span class="na">download</span><span class="pi">:</span> <span class="s">s3://xxxx/go-memory-leak-54d6d47d7-6hl64-7-8080-2024-11-11-03-49.heap to ./go-memory-leak-54d6d47d7-6hl64-7-8080-2024-11-11-03-49.heap</span>

<span class="s">go tool pprof -http=:8081 go-memory-leak-54d6d47d7-6hl64-7-8080-2024-11-11-03-49.heap</span>
</code></pre></div></div>

<p>Review the memory heap from the web interface provided by go tool pprof.
<img src="/assets/img/2024-11-11-oom-kill/pprof.png" alt="pprof" title="pprof" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>Using go tool pprof, we can manually capture memory heaps during OOM events. Setting up the pprof wrapper for on-demand heaps is relatively straightforward. However, capturing a heap at a specific high-memory usage moment can be challenging. For these cases, monitoring tools that automatically generate/store heaps can be invaluable, making the csepulveda/oom-heap-dumper sidecar an effective tool for managing memory dumps in high-demand situations.</p>

                <div class="page-footer">

                </div>
                <section class="author-box">
  <div class="author-left">
    <img src="/assets/img/profile-photo.jpg" alt="César Sepúlveda" class="author-img">
    <ul class="contact-links">
    
    <li class="email" data-balloon="Email" data-balloon-pos="left"><a href="mailto:cesar.sepulveda@gmail.com"><i class="far fa-envelope"></i></a></li>
    

    
    <li class="linkedin" data-balloon="LinkedIn" data-balloon-pos="left"><a href="https://in.linkedin.com/in/csepulvedab" target="_blank"><i
            class="fab fa-linkedin"></i></a></li>
    

    
    <li class="github" data-balloon="GitHub" data-balloon-pos="right"><a href="http://github.com/csepulveda" target="_blank"><i
            class="fab fa-github"></i></a></li>
    

    
    

    
    
</ul>
  </div>
  <div class="author-right">
    <h2>César Sepúlveda</h2>
    <p>I am a DevOps/SRE leader with over 13 years of experience, managing infrastructure and security to ensure scalability, reliability, and compliance.</p>

  </div>
</section>

                <div class="recent-box">
  <h2 class="recent-title">Recent Posts</h2>
  <div class="recent-list">
    
      
        <a href="/oom-kill/" class="recent-item" style="background: url(/assets/img/2024-11-11-oom-kill/oom.png) center no-repeat;"><span>How to Troubleshoot OOM Issues in Go Applications Running on Kubernetes</span></a>
      
    
      
        <a href="/p14-to-p2/" class="recent-item" style="background: url(/assets/img/2024-08-26-p14-to-p2/post.png) center no-repeat;"><span>Cad Cartagena - P14 to P2</span></a>
      
    
  </div>
</div> <!-- End Recent-Box -->

            </div>
        </div> <!-- End Wrapper -->
    </article>
<script src="//unpkg.com/isotope-layout@3.0.6/dist/isotope.pkgd.min.js"></script>
<script src="//unpkg.com/isotope-packery@2.0.1/packery-mode.pkgd.js"></script>
<script src="//unpkg.com/imagesloaded@4.1.4/imagesloaded.pkgd.js"></script>
<script src="/assets/js/main.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YPBW8TZ5KS"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YPBW8TZ5KS');
</script>

</div>

      <script src="//unpkg.com/isotope-layout@3.0.6/dist/isotope.pkgd.min.js"></script>
<script src="//unpkg.com/isotope-packery@2.0.1/packery-mode.pkgd.js"></script>
<script src="//unpkg.com/imagesloaded@4.1.4/imagesloaded.pkgd.js"></script>
<script src="/assets/js/main.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YPBW8TZ5KS"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YPBW8TZ5KS');
</script>

  </body>
</html>